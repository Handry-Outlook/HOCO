import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import imageio.v2 as imageio
from matplotlib.colors import LinearSegmentedColormap
from matplotlib.patches import Polygon
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from matplotlib.transforms import Bbox
from matplotlib.path import Path
from matplotlib.patches import Rectangle
from matplotlib import image
import json
import requests
import boto3
import argparse
import uuid
from datetime import datetime, timedelta
import time
from dotenv import load_dotenv

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# --- Environment Setup ---
load_dotenv()
MAPBOX_ACCESS_TOKEN = os.getenv("pk.eyJ1IjoiaGFuZHJ5MjAxOTEwMjYiLCJhIjoiY21jYWg2eG1lMDNkYTJxczVyOWM5bW9mciJ9.bA-qxiByTB_RseY1fgU4rg")
MAPBOX_USERNAME = os.getenv("handry20191026")

# --- Constants & Colors ---
reflectivity_colors = [
    (0x00, 0xb4, 0xe7), (0x00, 0xa4, 0xed), (0x00, 0x94, 0xf3), (0x00, 0x84, 0xf9),
    (0x00, 0x74, 0xff), (0x00, 0x84, 0xf9), (0x00, 0x50, 0xff), (0x00, 0x3c, 0xff),
    (0x00, 0x28, 0xff), (0x00, 0x14, 0xff), (0x00, 0x00, 0xff), (0x00, 0x00, 0xff),
    (0x00, 0x00, 0xeb), (0x00, 0x00, 0xe1), (0x00, 0x00, 0xd7), (0x00, 0x00, 0xcd),
    (0x00, 0x00, 0xc2), (0x00, 0x00, 0xb7), (0x00, 0x00, 0xab), (0x00, 0x00, 0x9e),
    (0xff, 0xc8, 0xff), (0xf4, 0xb4, 0xf4), (0xe8, 0xa0, 0xe8), (0xdd, 0x8c, 0xdd),
    (0xd1, 0x78, 0xd1), (0xc6, 0x64, 0xc6), (0xba, 0x50, 0xba), (0xaf, 0x3c, 0xaf),
    (0xa3, 0x28, 0xa3), (0x98, 0x14, 0x98), (0x8c, 0x00, 0x8c)
]

lightning_colors2 = [
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff",
    "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff", "#ffffff"
]

lightning_colors = [
    "#ffffff", "#d0cece", "#d0cece", "#d0cece", "#d0cece", "#bea497", "#bea497", "#bea497", "#bea497", "#bea497",
    "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4",
    "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4", "#66c2a4",
    "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200",
    "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200", "#fff200",
    "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27",
    "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27", "#ff7f27",
    "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24",
    "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#ec1c24", "#b83dba", "#b83dba", "#b83dba", "#b83dba", "#b83dba",
    "#b83dba", "#b83dba", "#b83dba", "#b83dba", "#b83dba", "#b83dba", "#b83dba", "#b83dba", "#b83dba", "#b83dba"
]

supercell_colors = [
    "#ffffff", "#a5f7f7", "#a5f7f7", "#00ffff", "#00ffff", "#00ffff", "#00d6ce", "#00d6ce", "#00d6ce", "#00d6ce",
    "#00bd18", "#00bd18", "#00bd18", "#00bd18", "#00bd18", "#4ad621", "#4ad621", "#4ad621", "#4ad621", "#4ad621",
    "#a5e700", "#a5e700", "#a5e700", "#a5e700", "#a5e700", "#ffde00", "#ffde00", "#ffde00", "#ffde00", "#ffde00",
    "#ffad00", "#ffad00", "#ffad00", "#ffad00", "#ffad00", "#ff6300", "#ff6300", "#ff6300", "#ff6300", "#ff6300",
    "#ce3100", "#ce3100", "#ce3100", "#ce3100", "#ce3100", "#990066", "#990066", "#990066", "#990066", "#990066",
    "#dd01b6", "#dd01b6", "#dd01b6", "#dd01b6", "#dd01b6", "#ff15d6", "#ff15d6", "#ff15d6", "#ff15d6", "#ff15d6",
    "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec",
    "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec", "#ff91ec",
    "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff",
    "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff", "#ffe6ff"
]

tor_colors = supercell_colors.copy()

snow_colors = [
    "#ffffff", "#fcfeff", "#f9fcff", "#f6fbff", "#f3f9ff", "#f0f8ff", "#f0f8ff", "#edf9ff", "#eafbff", "#e6fcff",
    "#e3feff", "#e0ffff", "#e0ffff", "#d6f9fa", "#cdf3f5", "#c3ecf0", "#bae6eb", "#b0e0e6", "#b0e0e6", "#afdee6",
    "#afdde6", "#aedbe6", "#aedae6", "#add8e6", "#add8e6", "#a5d6ea", "#9ed4ee", "#96d2f2", "#8fd0f6", "#87cefa",
    "#87cefa", "#87cef6", "#87cef2", "#87ceef", "#87ceeb", "#87ceeb", "#65caf0", "#44c6f5", "#22c3fa", "#00bfff",
    "#00bfff", "#08b3ff", "#0fa8ff", "#169cff", "#1e90ff", "#1e90ff", "#3091fb", "#4192f6", "#5294f2", "#6495ed",
    "#6495ed", "#5b8aea", "#527fe7", "#4a74e4", "#4169e1", "#4169e1", "#314fe8", "#2034f0", "#101af8", "#0000ff",
    "#0000ff", "#0000e2", "#0000c5", "#0000a8", "#00008b", "#00008b", "#000088", "#000085", "#000083", "#000080",
    "#000080", "#06067c", "#0c0c78", "#131374", "#191970", "#191970", "#252277", "#302b7e", "#3c3484", "#483d8b",
    "#483d8b", "#50449c", "#594cac", "#6253bc", "#6a5acd", "#6a5acd", "#6e5ed5", "#7261de", "#7765e6", "#7b68ee",
    "#7b68ee", "#6e6ee0", "#6075d1", "#537bc2", "#4682b4"
]

MODELS = ['frahd', 'rpdid2', 'swisseu', 'ez', 'swissnow', 'ezswiss', 'ukmo2km', 'swissmrf', 'hardmi', 'deuhd', 'gfs-hd', 'multiceur']
WEATHER_TYPES = {'lightning': 275, 'tornado': 549, 'supercell': 242, 'reflectivity': 241, 'snow': 319}
REGIONS = {'UK': '18', 'England': '4855', 'Zoom-in England': '7523','Ireland and Northern Ireland': '949', 'Wales':'4858', 'Isle of Man' : '7433', 'Scotland':'7267'}

reflectivity_model_parameters = {
    'rpdid2': (2, 0.0007), 'swisseu': (5, 0.0035), 'ez': (3, 0.0005), 'swissnow': (5, 0.0035),
    'ezswiss': (5, 0.0035), 'ukmo2km': (2, 0.00025), 'swissmrf': (3, 0.001), 'frahd': (2, 0.0007)
}
lightning_model_parameters = {
    'rpdid2': (2, 0.001), 'swisseu': (5, 0.003), 'ez': (3, 0.00025), 'swissnow': (5, 0.003),
    'ezswiss': (5, 0.003), 'ukmo2km': (1, 0.00025), 'swissmrf': (3, 0.001), 'hardmi': (3, 0.002)
}
supercell_model_parameters = {
    'rpdid2': (2, 0.0007), 'swisseu': (5, 0.0035), 'ez': (3, 0.0005), 'swissnow': (5, 0.0035),
    'ezswiss': (5, 0.0035), 'ukmo2km': (2, 0.00025), 'swissmrf': (3, 0.001)
}
tor_model_parameters = supercell_model_parameters.copy()
snow_model_parameters = {
    'rpdid2': (5, 0.05), 'swisseu': (5, 0.05), 'ez': (5, 0.05), 'swissnow': (5, 0.05),
    'ezswiss': (5, 0.05), 'ukmo2km': (5, 0.05), 'swissmrf': (5, 0.05), 'frahd': (5, 0.05), 'hardmi': (5, 0.05),
    'deuhd': (5, 0.05), 'gfs-hd': (2, 0.05), 'multiceur': (4, 0.05)
}

model_weights = {
    'rpdid2': 0.8, 'swisseu': 1.1, 'ez': 1.1, 'swissnow': 0.9, 'ezswiss': 1,
    'ukmo2km': 0.9, 'swissmrf': 0.5, 'frahd': 0.8, 'hardmi': 1,
    'deuhd': 1, 'gfs-hd': 1, 'multiceur': 1
}

model_colors = {
    'rpdid2': (1.0, 0.0, 0.0), 'swisseu': (0.0, 1.0, 0.0), 'ez': (0.0, 0.0, 1.0),
    'swissnow': (1.0, 1.0, 0.0), 'ezswiss': (1.0, 0.0, 1.0), 'ukmo2km': (0.0, 1.0, 1.0),
    'swissmrf': (0.5, 0.0, 0.5), 'frahd': (1.0, 0.5, 0.0),
    'hardmi': (0.2, 0.8, 0.2), 'deuhd': (0.5, 0.5, 0.0), 'gfs-hd': (0.5, 0.5, 0.5), 'multiceur': (0.0, 0.5, 0.5)
}

lightning_custom_cmap = LinearSegmentedColormap.from_list("custom_lightning_cmap", lightning_colors, N=100)
lightning_custom_cmap2 = LinearSegmentedColormap.from_list("custom_lightning_cmap2", lightning_colors2, N=100)
supercell_custom_cmap = LinearSegmentedColormap.from_list("custom_supercell_cmap", supercell_colors, N=100)
tor_custom_cmap = LinearSegmentedColormap.from_list("custom_tor_cmap", tor_colors, N=100)
snow_custom_cmap = LinearSegmentedColormap.from_list("custom_snow_cmap", snow_colors, N=100)

# --- Core Math & Accumulation Logic ---
def extract_reflectivity_rate(image):
    mask = np.zeros(image.shape[:2], dtype=np.uint8)
    for color in reflectivity_colors:
        lower_bound = np.array([max(0, c - 5) for c in color], dtype=np.uint8)
        upper_bound = np.array([min(255, c + 5) for c in color], dtype=np.uint8)
        color_mask = cv2.inRange(image, lower_bound, upper_bound)
        mask = cv2.bitwise_or(mask, color_mask)
    return cv2.bitwise_and(image, image, mask=mask)

def accumulate_reflectivity_rates(image_paths, model_parameters, weights):
    model_data = {model: None for model in MODELS}
    total_frames = 0
    model_radius = {}
    for path in image_paths:
        model_match = next((m for m in sorted(model_parameters.keys(), key=len, reverse=True) if m.lower() in path.lower()), None)
        if not model_match: continue
        radius, risk_factor = model_parameters[model_match]
        if model_match not in model_radius: model_radius[model_match] = radius
        frames = [cv2.cvtColor(f, cv2.COLOR_RGB2BGR) for f in imageio.get_reader(path)] if path.endswith('.gif') else [cv2.imread(path)]
        for frame_bgr in frames:
            total_frames += 1
            gray_reflectivity_rate = cv2.cvtColor(extract_reflectivity_rate(frame_bgr[:620, :]), cv2.COLOR_BGR2GRAY)
            weighted_data = gray_reflectivity_rate * risk_factor * weights[model_match]
            if model_data[model_match] is None: model_data[model_match] = np.zeros_like(gray_reflectivity_rate, dtype=np.float32)
            model_data[model_match] += weighted_data
    for model in model_data:
        if model_data[model] is not None and total_frames > 0: model_data[model] /= total_frames
    return model_data, total_frames, model_radius

def calculate_reflectivity_risk(model_data):
    return {model: (data > 0).astype(np.uint8) * 255 if data is not None else None for model, data in model_data.items()}

def extract_lightning_rate(image):
    mask = cv2.inRange(cv2.cvtColor(image, cv2.COLOR_BGR2HSV), np.array([0, 50, 50]), np.array([255, 255, 255]))
    return cv2.bitwise_and(image, image, mask=mask)

def extract_hardmi_lightning_rate(image):
    tol = 5
    lightning_rate = np.zeros(image.shape[:2], dtype=np.uint8)
    lightning_rate[cv2.inRange(image, np.array([255, 95, 253]) - tol, np.array([255, 95, 253]) + tol) > 0] = 15
    lightning_rate[cv2.inRange(image, np.array([188, 26, 186]) - tol, np.array([188, 26, 186]) + tol) > 0] = 100
    return lightning_rate

def accumulate_lightning_rates(image_paths, model_parameters, weights):
    model_data = {model: {'data': None, 'frames': 0, 'radius': None} for model in MODELS}
    for path in image_paths:
        model_match = next((m for m in sorted(model_parameters.keys(), key=len, reverse=True) if m.lower() in path.lower()), None)
        if not model_match: continue
        radius, risk_factor = model_parameters[model_match]
        model_data[model_match]['radius'] = radius
        frames = [cv2.cvtColor(f, cv2.COLOR_RGB2BGR) for f in imageio.get_reader(path)] if path.endswith('.gif') else [cv2.imread(path)]
        for frame_bgr in frames:
            model_data[model_match]['frames'] += 1
            cropped_frame = frame_bgr[:620, :]
            gray_lightning_rate = extract_hardmi_lightning_rate(cropped_frame) if model_match == 'hardmi' else cv2.cvtColor(extract_lightning_rate(cropped_frame), cv2.COLOR_BGR2GRAY)
            weighted_data = gray_lightning_rate * risk_factor * weights[model_match]
            if model_data[model_match]['data'] is None: model_data[model_match]['data'] = np.zeros_like(gray_lightning_rate, dtype=np.float32)
            model_data[model_match]['data'] += weighted_data
    return model_data

def calculate_thunderstorm_risk(data, model_radius, total_frames, region, ppm=5):
    if region.lower() == "uk": ppm = 4
    if total_frames > 0: data = data / (total_frames ** 0.425)
    smoothed_data = cv2.GaussianBlur(data, (21, 21) if region.lower() == "uk" else (29, 29), 150)
    risk_spread = np.zeros_like(smoothed_data, dtype=np.float32)
    for model, radius in model_radius.items():
        kernel = np.zeros((2 * radius * ppm + 1, 2 * radius * ppm + 1), np.float32)
        cv2.circle(kernel, (radius * ppm, radius * ppm), radius * ppm, 1, thickness=-1)
        model_risk_spread = np.log1p(cv2.filter2D(smoothed_data, -1, kernel))
        risk_spread = np.maximum(risk_spread, model_risk_spread)
    return risk_spread

def extract_supercell_rate(image, color_ranges):
    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    return {rl: cv2.bitwise_and(image, image, mask=cv2.inRange(hsv_image, lb, ub)) for rl, (lb, ub) in color_ranges.items()}

def accumulate_supercell_rates(image_paths, model_parameters, weights):
    accumulated_data, total_frames, model_radius = None, 0, {}
    color_ranges = {
        '0.2': (np.array([117, 186, 255]), np.array([117, 186, 255])),
        '0.5': (np.array([0, 105, 210]), np.array([0, 105, 210])),
        '1': (np.array([20, 143, 127]), np.array([20, 143, 127])),
        '2': (np.array([99, 237, 7]), np.array([99, 237, 7])),
        '3': (np.array([255, 244, 43]), np.array([255, 244, 43])),
        '4': (np.array([232, 220, 0]), np.array([232, 220, 0])),
        '6': (np.array([255, 127, 39]), np.array([255, 127, 39])),
        '8': (np.array([247, 30, 84]), np.array([247, 30, 84])),
        '10': (np.array([136, 0, 0]), np.array([136, 0, 0])),
        '15': (np.array([100, 0, 127]), np.array([100, 0, 127])),
        '20': (np.array([194, 0, 251]), np.array([194, 0, 251])),
        '25': (np.array([221, 102, 255]), np.array([221, 102, 255])),
        '30': (np.array([235, 166, 255]), np.array([235, 166, 255])),
        '40': (np.array([185, 122, 87]), np.array([185, 122, 87])),
    }
    for path in image_paths:
        model_match = next((m for m in sorted(model_parameters.keys(), key=len, reverse=True) if m.lower() in path.lower()), None)
        if not model_match: continue
        radius, risk_factor = model_parameters[model_match]
        model_radius[model_match] = radius
        frames = [cv2.cvtColor(f, cv2.COLOR_RGB2BGR) for f in imageio.get_reader(path)] if path.endswith('.gif') else [cv2.imread(path)]
        for frame_bgr in frames:
            total_frames += 1
            if accumulated_data is None: accumulated_data = np.zeros_like(frame_bgr[:620, :, 0], dtype=np.float32)
            for rl, rate in extract_supercell_rate(frame_bgr[:620, :], color_ranges).items():
                accumulated_data += cv2.cvtColor(rate, cv2.COLOR_BGR2GRAY) * risk_factor * weights[model_match]
    return accumulated_data, total_frames, model_radius

def calculate_supercell_risk(data, model_radius, total_frames, region, ppm=1):
    if region.lower() == "uk": ppm = 4
    if total_frames > 0: data = data / (total_frames ** 0.425) / 3000 / 17.5
    smoothed_data = cv2.GaussianBlur(data, (21, 21) if region.lower() == "uk" else (29, 29), 150)
    risk_spread = np.zeros_like(smoothed_data, dtype=np.float32)
    for model, radius in model_radius.items():
        kernel = np.zeros((2 * radius * ppm + 1, 2 * radius * ppm + 1), np.float32)
        cv2.circle(kernel, (radius * ppm, radius * ppm), radius * ppm, 1, thickness=-1)
        model_risk_spread = np.log1p(cv2.filter2D(smoothed_data, -1, kernel))
        risk_spread = np.maximum(risk_spread, model_risk_spread)
    return np.log1p(risk_spread * 0.6) * 6

def extract_tor_rate(image, color_ranges): return extract_supercell_rate(image, color_ranges)

def accumulate_tor_rates(image_paths, model_parameters, weights):
    accumulated_data, total_frames, model_radius = None, 0, {}
    color_ranges = {
        '0.1': (np.array([10, 10, 10]), np.array([255, 255, 255])), '0.2': (np.array([117, 186, 255]), np.array([117, 186, 255])),
        '0.3': (np.array([4, 130, 255]), np.array([4, 130, 255])), '0.4': (np.array([0, 105, 210]), np.array([0, 105, 210])),
        '0.5': (np.array([0, 54, 127]), np.array([0, 54, 127])), '0.6': (np.array([20, 143, 127]), np.array([20, 143, 127])),
        '0.8': (np.array([99, 237, 7]), np.array([99, 237, 7])), '1': (np.array([255, 244, 43]), np.array([255, 244, 43])),
        '1.25': (np.array([232, 220, 0]), np.array([232, 220, 0])), '1.5': (np.array([255, 127, 39]), np.array([255, 127, 39])),
        '1.75': (np.array([247, 30, 84]), np.array([247, 30, 84])), '2': (np.array([136, 0, 0]), np.array([136, 0, 0])),
        '2.5': (np.array([100, 0, 127]), np.array([100, 0, 127])), '3': (np.array([194, 0, 251]), np.array([194, 0, 251])),
        '4': (np.array([221, 102, 255]), np.array([221, 102, 255])), '6': (np.array([235, 166, 255]), np.array([235, 166, 255])),
        '8': (np.array([185, 122, 87]), np.array([185, 122, 87]))
    }
    for path in image_paths:
        model_match = next((m for m in sorted(model_parameters.keys(), key=len, reverse=True) if m.lower() in path.lower()), None)
        if not model_match: continue
        radius, risk_factor = model_parameters[model_match]
        model_radius[model_match] = radius
        frames = [cv2.cvtColor(f, cv2.COLOR_RGB2BGR) for f in imageio.get_reader(path)] if path.endswith('.gif') else [cv2.imread(path)]
        for frame_bgr in frames:
            total_frames += 1
            if accumulated_data is None: accumulated_data = np.zeros_like(frame_bgr[:620, :, 0], dtype=np.float32)
            for rl, rate in extract_tor_rate(frame_bgr[:620, :], color_ranges).items():
                accumulated_data += cv2.cvtColor(rate, cv2.COLOR_BGR2GRAY) * risk_factor * weights[model_match]
    return accumulated_data, total_frames, model_radius

def calculate_tor_risk(data, model_radius, total_frames, region, ppm=1):
    if region.lower() == "uk": ppm = 4
    if total_frames > 0: data = data / (total_frames ** 0.425) / 3000 / 15
    smoothed_data = cv2.GaussianBlur(data, (21, 21) if region.lower() == "uk" else (29, 29), 150)
    risk_spread = np.zeros_like(smoothed_data, dtype=np.float32)
    for model, radius in model_radius.items():
        kernel = np.zeros((2 * radius * ppm + 1, 2 * radius * ppm + 1), np.float32)
        cv2.circle(kernel, (radius * ppm, radius * ppm), radius * ppm, 1, thickness=-1)
        model_risk_spread = np.log1p(cv2.filter2D(smoothed_data, -1, kernel))
        risk_spread = np.maximum(risk_spread, model_risk_spread)
    return np.log1p(risk_spread * 0.3) * 3

def extract_snow_rate(image):
    color_ranges = {
        '0.1': (np.array([10, 10, 10]), np.array([255, 255, 255])), '0.2': (np.array([117, 186, 255]), np.array([117, 186, 255])),
        '0.3': (np.array([4, 130, 255]), np.array([4, 130, 255])), '0.4': (np.array([0, 105, 210]), np.array([0, 105, 210])),
        '0.5': (np.array([0, 54, 127]), np.array([0, 54, 127])), '0.6': (np.array([20, 143, 127]), np.array([20, 143, 127])),
        '0.8': (np.array([99, 237, 7]), np.array([99, 237, 7])), '1': (np.array([255, 244, 43]), np.array([255, 244, 43])),
        '1.25': (np.array([232, 220, 0]), np.array([232, 220, 0])), '1.5': (np.array([255, 127, 39]), np.array([255, 127, 39])),
        '1.75': (np.array([247, 30, 84]), np.array([247, 30, 84])), '2': (np.array([136, 0, 0]), np.array([136, 0, 0])),
        '2.5': (np.array([100, 0, 127]), np.array([100, 0, 127])), '3': (np.array([194, 0, 251]), np.array([194, 0, 251])),
        '4': (np.array([221, 102, 255]), np.array([221, 102, 255])), '6': (np.array([235, 166, 255]), np.array([235, 166, 255])),
        '8': (np.array([185, 122, 87]), np.array([185, 122, 87]))
    }
    return extract_supercell_rate(image, color_ranges)

def accumulate_snow_rates(image_paths, model_parameters, weights):
    accumulated_data, total_frames, model_radius = None, 0, {}
    for path in image_paths:
        model_match = next((m for m in sorted(model_parameters.keys(), key=len, reverse=True) if m.lower() in path.lower()), None)
        if not model_match: continue
        radius, risk_factor = model_parameters[model_match]
        model_radius[model_match] = radius
        frames = [cv2.cvtColor(f, cv2.COLOR_RGB2BGR) for f in imageio.get_reader(path)] if path.endswith('.gif') else [cv2.imread(path)]
        for frame_bgr in frames:
            total_frames += 1
            if accumulated_data is None: accumulated_data = np.zeros_like(frame_bgr[:620, :, 0], dtype=np.float32)
            for rl, rate in extract_snow_rate(frame_bgr[:620, :]).items():
                accumulated_data += cv2.cvtColor(rate, cv2.COLOR_BGR2GRAY) * risk_factor * weights[model_match]
    return accumulated_data, total_frames, model_radius

def calculate_snow_risk(data, model_radius, total_frames, region, ppm=0.5):
    import math
    if region == "uk": ppm = 0.4
    if total_frames > 0: data = (data / (total_frames ** 0.425)) / 10
    smoothed_data = cv2.GaussianBlur(data, (21, 21) if region == "uk" else (15, 15), 150 if region == "uk" else 20)
    risk_spread = np.zeros_like(smoothed_data, dtype=np.float32)
    for model, radius in model_radius.items():
        kernel_radius = radius * ppm
        kernel_size = 2 * math.ceil(kernel_radius) + 1
        kernel = np.zeros((kernel_size, kernel_size), np.float32)
        center = kernel_size // 2
        cv2.circle(kernel, (center, center), math.ceil(kernel_radius), 1, thickness=-1)
        model_risk_spread = np.log1p(cv2.filter2D(smoothed_data, -1, kernel))
        risk_spread = np.maximum(risk_spread, model_risk_spread)
    return risk_spread

# --- Mapbox GeoJSON Export ---
def export_risk_to_geojson(data_array, calc_factor, output_filepath, bounds=None):
    """Exports the raw numpy risk data array to a GeoJSON format suitable for Mapbox."""
    percentage_data = (data_array * calc_factor) * 100
    y_indices, x_indices = np.where(percentage_data >= 1.0)
    height, width = data_array.shape
    features = []
    
    if bounds:
        min_lon, min_lat, max_lon, max_lat = bounds
        lon_step = (max_lon - min_lon) / width
        lat_step = (max_lat - min_lat) / height
        
    for y, x in zip(y_indices, x_indices):
        val = float(percentage_data[y, x])
        if bounds:
            lon = min_lon + (x * lon_step)
            lat = max_lat - (y * lat_step)
            features.append({
                "type": "Feature",
                "geometry": {"type": "Point", "coordinates": [lon, lat]},
                "properties": {"risk_pct": round(val, 2)}
            })
        else:
            features.append({
                "type": "Feature",
                "geometry": {"type": "Point", "coordinates": [float(x), float(y)]},
                "properties": {"risk_pct": round(val, 2), "pixel_x": int(x), "pixel_y": int(y)}
            })
            
    with open(output_filepath, 'w') as f:
        json.dump({"type": "FeatureCollection", "features": features}, f)
    return output_filepath

def upload_to_mapbox(filepath, tileset_name):
    """
    Handles the Mapbox Uploads API workflow with detailed logging.
    1. Fetches temporary S3 credentials from Mapbox.
    2. Uploads the GeoJSON file to the Mapbox S3 staging bucket.
    3. Notifies Mapbox to begin processing the file into a tileset.
    """
    # FIX: These variables MUST match the names in your GitHub Secrets
    # We fetch them inside the function to ensure they are fresh
    token = os.getenv("MAPBOX_ACCESS_TOKEN")
    user = os.getenv("MAPBOX_USERNAME")

    if not token or not user:
        print("CRITICAL ERROR: Mapbox credentials missing from Environment!")
        print(f"DEBUG: Username set: {bool(user)} | Token set: {bool(token)}")
        return None

    print(f"\n--- Starting Mapbox Upload: {tileset_name} ---")

    # Step 1: Request S3 staging credentials from Mapbox
    print("Step 1: Requesting temporary S3 credentials from Mapbox API...")
    creds_url = f"https://api.mapbox.com/uploads/v1/{user}/credentials?access_token={token}"
    
    try:
        resp = requests.get(creds_url)
        if resp.status_code != 200:
            print(f"FAILED Step 1: {resp.status_code} - {resp.text}")
            return None
        creds = resp.json()
        print("Successfully obtained S3 credentials.")

        # Step 2: Upload the local file to the Mapbox S3 staging bucket
        print(f"Step 2: Uploading {os.path.basename(filepath)} to Mapbox S3 staging...")
        s3_client = boto3.client(
            's3',
            aws_access_key_id=creds['accessKeyId'],
            aws_secret_access_key=creds['secretAccessKey'],
            aws_session_token=creds['sessionToken']
        )
        s3_client.upload_file(filepath, creds['bucket'], creds['key'])
        print("S3 Upload successful.")

        # Step 3: Tell Mapbox to create the Tileset from the S3 file
        print("Step 3: Notifying Mapbox to begin tileset processing...")
        upload_url = f"https://api.mapbox.com/uploads/v1/{user}?access_token={token}"
        payload = {
            "tileset": f"{user}.{tileset_name}",
            "url": f"http://{creds['bucket']}.s3.amazonaws.com/{creds['key']}"
        }
        
        final_resp = requests.post(upload_url, json=payload)
        if final_resp.status_code == 201:
            upload_id = final_resp.json().get('id')
            print(f"SUCCESS! Mapbox job created. Upload ID: {upload_id}")
            print(f"Your tileset ID will be: {user}.{tileset_name}")
            return final_resp.json()
        else:
            print(f"FAILED Step 3: {final_resp.status_code} - {final_resp.text}")
            return None

    except Exception as e:
        print(f"UNEXPECTED ERROR during upload: {str(e)}")
        return None

# --- Orchestration & Headless Selenium Execution ---
def get_available_runs(model, start_time):
    interval = 3 if model in ['rpdid2', 'frahd', 'hardmi', 'swisseu'] else 1 if model == 'swissnow' else 6
    run_times = []
    current_time = datetime.now()
    reference_time = start_time if start_time < current_time else current_time
    for i in range(30):
        run_time = reference_time - timedelta(hours=i * interval)
        if model in ['rpdid2', 'frahd', 'hardmi', 'swisseu']:
            run_time = run_time.replace(hour=(run_time.hour // 3) * 3, minute=0, second=0, microsecond=0)
        elif model != 'swissnow':
            run_time = run_time.replace(hour=(run_time.hour // 6) * 6, minute=0, second=0, microsecond=0)
        else:
            run_time = run_time.replace(minute=0, second=0, microsecond=0)
        if run_time not in run_times:
            run_times.append(run_time)
    return sorted(run_times, reverse=True)[:5] # Taking the top 5 runs for headless

def process_weather_data(job_uuid, start_time, end_time, region, weather_params, weights=model_weights):
    print(f"Starting Job {job_uuid} | Region: {region} | Times: {start_time} to {end_time}")
    base_url = "https://img5.meteologix.com/images/data/cache/model/model_mod{model_name}_{runtime}_{hours}_{region_code}_{weather_type}.png"
    
    models_dict = {model: get_available_runs(model, start_time) for model in MODELS}
    image_paths = {param: [] for param in WEATHER_TYPES.keys()}
    
    # 1. Setup Headless WebDriver
    print("Initializing headless Chrome...")
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    # Simplified Map of URLs
    model_chart_urls = {
        "ez": {"lightning": "https://meteologix.com/uk/model-charts/euro/united-kingdom/lightning-density/20250320-1800z.html", "snow": "https://meteologix.com/uk/model-charts/euro/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html", "reflectivity": "https://meteologix.com/uk/model-charts/euro/united-kingdom/reflectivity/20250320-1800z.html"},
        "swisseu": {"lightning": "https://meteologix.com/uk/model-charts/swisshd-eu/united-kingdom/lightning-flash-rate/20250320-1800z.html", "supercell": "https://meteologix.com/uk/model-charts/swisshd-eu/united-kingdom/supercell-composite/20250320-1800z.html", "tornado": "https://meteologix.com/uk/model-charts/swisshd-eu/united-kingdom/sign-tornado-param/20250320-1800z.html", "snow": "https://meteologix.com/uk/model-charts/swisshd-eu/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html", "reflectivity": "https://meteologix.com/uk/model-charts/swisshd-eu/united-kingdom/base-reflectivity/20250320-1800z.html"},
        "swissnow": {"lightning": "https://meteologix.com/uk/model-charts/swisshd-nowcast/united-kingdom/lightning-flash-rate/20250320-1800z.html", "supercell": "https://meteologix.com/uk/model-charts/swisshd-nowcast/united-kingdom/supercell-composite/20250320-1800z.html", "tornado": "https://meteologix.com/uk/model-charts/swisshd-nowcast/united-kingdom/sign-tornado-param/20250320-1800z.html", "snow": "https://meteologix.com/uk/model-charts/swisshd-nowcast/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html", "reflectivity": "https://meteologix.com/uk/model-charts/swisshd-nowcast/united-kingdom/base-reflectivity/20250320-1800z.html"},
        "ezswiss": {"lightning": "https://meteologix.com/uk/model-charts/swisshd-ecmwf/united-kingdom/lightning-flash-rate/20250320-1800z.html", "supercell": "https://meteologix.com/uk/model-charts/swisshd-ecmwf/united-kingdom/supercell-composite/20250320-1800z.html", "tornado": "https://meteologix.com/uk/model-charts/swisshd-ecmwf/united-kingdom/sign-tornado-param/20250320-1800z.html", "snow": "https://meteologix.com/uk/model-charts/swisshd-ecmwf/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html", "reflectivity": "https://meteologix.com/uk/model-charts/swisshd-ecmwf/united-kingdom/base-reflectivity/20250320-1800z.html"},
        "ukmo2km": {"lightning": "https://meteologix.com/uk/model-charts/ukmo-2km/united-kingdom/lightning-flash-rate/20250320-2200z.html", "snow": "https://meteologix.com/uk/model-charts/ukmo-2km/united-kingdom/precipitation-snow-1h--en/20250320-2200z.html"},
        "swissmrf": {"lightning": "https://meteologix.com/uk/model-charts/swiss-mrf/united-kingdom/lightning-flash-rate/20250320-1800z.html", "tornado": "https://meteologix.com/uk/model-charts/swiss-mrf/united-kingdom/sign-tornado-param/20250320-1800z.html", "snow": "https://meteologix.com/uk/model-charts/swiss-mrf/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html", "reflectivity": "https://meteologix.com/uk/model-charts/swiss-mrf/united-kingdom/base-reflectivity/20250320-1800z.html"},
        "rpdid2": {"lightning": "https://meteologix.com/uk/model-charts/rapid-id2/england/lightning-flash-rate/20250320-2200z.html", "snow": "https://meteologix.com/uk/model-charts/rapid-id2/england/precipitation-snow-1h--en/20250320-2200z.html", "reflectivity": "https://meteologix.com/uk/model-charts/rapid-id2/england/base-reflectivity/20250320-2200z.html"},
        "frahd": {"snow": "https://meteologix.com/uk/model-charts/french-hd/england/precipitation-snow-1h--en/20250320-2200z.html", "reflectivity": "https://meteologix.com/uk/model-charts/french-hd/england/base-reflectivity/20250320-2200z.html"},
        "hardmi": {"lightning": "https://meteologix.com/uk/model-charts/harmonie-dmi/united-kingdom/lightning-flash-rate/20250320-1800z.html", "snow": "https://meteologix.com/uk/model-charts/harmonie-dmi/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html"},
        "deuhd": {"snow": "https://meteologix.com/uk/model-charts/deu-hd/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html"},
        "gfs-hd": {"snow": "https://meteologix.com/uk/model-charts/gfs-hd/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html"},
        "multiceur": {"snow": "https://meteologix.com/uk/model-charts/multimodel-eur/united-kingdom/precipitation-snow-1h--en/20250320-1800z.html"}
    }

    print("Fetching images...")
    for model, run_times in models_dict.items():
        for weather in weather_params:
            chart_url = model_chart_urls.get(model, {}).get(weather)
            if not chart_url: continue
            
            try:
                driver.get(chart_url)
                time.sleep(1)
                if "Access Denied" in driver.page_source: continue
                cookies = {cookie['name']: cookie['value'] for cookie in driver.get_cookies()}
            except Exception as e:
                print(f"Failed loading driver for {model}: {str(e)}")
                continue

            session = requests.Session()
            session.headers.update({"User-Agent": "Mozilla/5.0", "Referer": chart_url})
            session.cookies.update(cookies)

            for run_time in run_times:
                for hour in range(max(0, int((start_time - run_time).total_seconds() / 3600)), int((end_time - run_time).total_seconds() / 3600) + 1):
                    weather_type = 547 if (model == 'ez' and weather == 'lightning') else 155 if (model == 'hardmi' and weather == 'lightning') else WEATHER_TYPES[weather]
                    url = base_url.format(model_name=model, runtime=run_time.strftime("%Y%m%d%H"), hours=f"{hour:02d}", region_code=REGIONS[region], weather_type=weather_type)
                    
                    temp_output_dir = f"{weather}_images"
                    os.makedirs(temp_output_dir, exist_ok=True)
                    output_path = os.path.join(temp_output_dir, f"{model}_{run_time.strftime('%Y%m%d%H')}_{hour:02d}_{weather}_{region}.png")
                    
                    if not os.path.exists(output_path):
                        try:
                            response = session.get(url, timeout=10)
                            if response.status_code == 200:
                                with open(output_path, 'wb') as f: f.write(response.content)
                                image_paths[weather].append(output_path)
                        except Exception as e:
                            pass
                    else:
                        image_paths[weather].append(output_path)
    
    driver.quit()
    print("Image fetching complete. Starting Processing...")

    # 2. Setup processing vars
    output_dir = os.path.join("HOCO_Backend_Out", start_time.strftime("%d%m%Y"))
    os.makedirs(output_dir, exist_ok=True)
    bounds_map = {"UK": [-11.0, 49.0, 3.0, 61.0]} # Extent map for GeoJSON alignment
    region_bounds = bounds_map.get(region, [-11.0, 49.0, 3.0, 61.0])

    # 3. Processing routines
    if 'lightning' in weather_params and image_paths['lightning']:
        model_data = accumulate_lightning_rates(image_paths['lightning'], lightning_model_parameters, weights)
        acc_lightning, total_frames, model_radius = None, 0, {}
        for model, data in model_data.items():
            if data['data'] is not None:
                if acc_lightning is None: acc_lightning = np.zeros_like(data['data'], dtype=np.float32)
                acc_lightning += data['data']
                total_frames += int(data['frames'])
                model_radius[model] = data['radius']
        
        if acc_lightning is not None:
            lightning_risk = calculate_thunderstorm_risk(acc_lightning, model_radius, total_frames, region)
            geojson_path = os.path.join(output_dir, f"{job_uuid}_lightning.geojson")
            export_risk_to_geojson(lightning_risk, 0.2, geojson_path, region_bounds)
            upload_to_mapbox(geojson_path, f"lightning_{job_uuid}")
            print(f"Processed Lightning: {geojson_path}")

    if 'supercell' in weather_params and image_paths['supercell']:
        acc_supercell, total_frames, model_radius = accumulate_supercell_rates(image_paths['supercell'], supercell_model_parameters, weights)
        if acc_supercell is not None:
            supercell_risk = calculate_supercell_risk(acc_supercell, model_radius, total_frames, region)
            geojson_path = os.path.join(output_dir, f"{job_uuid}_supercell.geojson")
            export_risk_to_geojson(supercell_risk, 12.5, geojson_path, region_bounds)
            upload_to_mapbox(geojson_path, f"supercell_{job_uuid}")
            print(f"Processed Supercell: {geojson_path}")

    if 'tornado' in weather_params and image_paths['tornado']:
        acc_tor, total_frames, model_radius = accumulate_tor_rates(image_paths['tornado'], tor_model_parameters, weights)
        if acc_tor is not None:
            tor_risk = calculate_tor_risk(acc_tor, model_radius, total_frames, region)
            geojson_path = os.path.join(output_dir, f"{job_uuid}_tornado.geojson")
            export_risk_to_geojson(tor_risk, 25.0, geojson_path, region_bounds)
            upload_to_mapbox(geojson_path, f"tornado_{job_uuid}")
            print(f"Processed Tornado: {geojson_path}")

    if 'snow' in weather_params and image_paths['snow']:
        acc_snow, total_frames, model_radius = accumulate_snow_rates(image_paths['snow'], snow_model_parameters, weights)
        if acc_snow is not None:
            snow_risk = calculate_snow_risk(acc_snow, model_radius, total_frames, region)
            geojson_path = os.path.join(output_dir, f"{job_uuid}_snow.geojson")
            export_risk_to_geojson(snow_risk, 0.2, geojson_path, region_bounds)
            upload_to_mapbox(geojson_path, f"snow_{job_uuid}")
            print(f"Processed Snow: {geojson_path}")

    print(f"Job {job_uuid} complete!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Headless Weather Risk Processor")
    parser.add_argument("--uuid", type=str, default=str(uuid.uuid4()), help="Unique job identifier")
    parser.add_argument("--start", type=str, required=True, help="Start time (DD/MM/YYYY HH:MM)")
    parser.add_argument("--end", type=str, required=True, help="End time (DD/MM/YYYY HH:MM)")
    parser.add_argument("--region", type=str, default="UK", choices=REGIONS.keys(), help="Target region")
    parser.add_argument("--weather", nargs='+', required=True, choices=WEATHER_TYPES.keys(), help="Weather parameters")
    
    # NEW: Argument to receive the JSON weights from GitHub/Frontend
    parser.add_argument("--weights", type=str, default="{}", help="JSON string of model weights")

    args = parser.parse_args()
    
    # Parse the weights JSON
    try:
        user_weights = json.loads(args.weights)
        # If the user passed an empty {} or specific models, 
        # we merge them with your existing model_weights or replace them
        if not user_weights:
            active_weights = model_weights
        else:
            # Use only models provided by the user
            active_weights = user_weights
            print(f"Applying custom weights: {active_weights}")
    except json.JSONDecodeError:
        print("Warning: Invalid weights JSON. Falling back to default model weights.")
        active_weights = model_weights

    # Convert date strings to datetime objects
    try:
        start_dt = datetime.strptime(args.start, "%d/%m/%Y %H:%M")
        end_dt = datetime.strptime(args.end, "%d/%m/%Y %H:%M")
    except ValueError as e:
        print(f"Error parsing dates: {e}. Ensure format is DD/MM/YYYY HH:MM")
        exit(1)
    
    # Trigger the main process with the dynamic weights
    process_weather_data(
        args.uuid, 
        start_dt, 
        end_dt, 
        args.region, 
        args.weather, 
        weights=active_weights
    )
